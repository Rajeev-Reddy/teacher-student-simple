{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nimport time\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-10T04:58:00.742927Z","iopub.execute_input":"2025-03-10T04:58:00.743237Z","iopub.status.idle":"2025-03-10T04:58:07.605175Z","shell.execute_reply.started":"2025-03-10T04:58:00.743182Z","shell.execute_reply":"2025-03-10T04:58:07.604300Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Transform: Convert images to tensors & normalize\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n\n# Load train & test datasets\ntrainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\ntestset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n\n# Create DataLoaders\ntrain_loader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T04:58:11.931508Z","iopub.execute_input":"2025-03-10T04:58:11.931821Z","iopub.status.idle":"2025-03-10T04:58:13.677085Z","shell.execute_reply.started":"2025-03-10T04:58:11.931793Z","shell.execute_reply":"2025-03-10T04:58:13.676168Z"}},"outputs":[{"name":"stdout","text":"Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nFailed to download (trying next):\nHTTP Error 404: Not Found\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 9.91M/9.91M [00:00<00:00, 58.4MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nFailed to download (trying next):\nHTTP Error 404: Not Found\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 28.9k/28.9k [00:00<00:00, 1.47MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nFailed to download (trying next):\nHTTP Error 404: Not Found\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1.65M/1.65M [00:00<00:00, 14.5MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nFailed to download (trying next):\nHTTP Error 404: Not Found\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4.54k/4.54k [00:00<00:00, 6.63MB/s]","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"class TeacherMLP(nn.Module):\n    def __init__(self):\n        super(TeacherMLP, self).__init__()\n        self.fc1 = nn.Linear(28 * 28, 512)  # Large hidden layer\n        self.fc2 = nn.Linear(512, 256)\n        self.fc3 = nn.Linear(256, 10)  # 10 output classes\n        self.relu = nn.ReLU()\n        \n    def forward(self, x):\n        x = x.view(-1, 28 * 28)  # Flatten input\n        x = self.relu(self.fc1(x))\n        x = self.relu(self.fc2(x))\n        x = self.fc3(x)  # No softmax, we use CrossEntropyLoss\n        return x\n\nteacher_model = TeacherMLP()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T04:59:46.085440Z","iopub.execute_input":"2025-03-10T04:59:46.085748Z","iopub.status.idle":"2025-03-10T04:59:46.104133Z","shell.execute_reply.started":"2025-03-10T04:59:46.085725Z","shell.execute_reply":"2025-03-10T04:59:46.103524Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def train_teacher(model, train_loader, epochs=5):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        for images, labels in train_loader:\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n        \n        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}\")\n\ntrain_teacher(teacher_model, train_loader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T05:00:15.583992Z","iopub.execute_input":"2025-03-10T05:00:15.584313Z","iopub.status.idle":"2025-03-10T05:01:34.057765Z","shell.execute_reply.started":"2025-03-10T05:00:15.584288Z","shell.execute_reply":"2025-03-10T05:01:34.056935Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 0.3007126267570486\nEpoch 2, Loss: 0.13330838035271841\nEpoch 3, Loss: 0.09999120620581736\nEpoch 4, Loss: 0.08134710471054464\nEpoch 5, Loss: 0.06779255375150901\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"class StudentMLP(nn.Module):\n    def __init__(self):\n        super(StudentMLP, self).__init__()\n        self.fc1 = nn.Linear(28 * 28, 128)  # Smaller hidden layer\n        self.fc2 = nn.Linear(128, 10)  # 10 output classes\n        self.relu = nn.ReLU()\n        \n    def forward(self, x):\n        x = x.view(-1, 28 * 28)  # Flatten input\n        x = self.relu(self.fc1(x))\n        x = self.fc2(x)  # No softmax, we use distillation loss\n        return x\n\nstudent_model = StudentMLP()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T05:10:15.179689Z","iopub.execute_input":"2025-03-10T05:10:15.180016Z","iopub.status.idle":"2025-03-10T05:10:15.186144Z","shell.execute_reply.started":"2025-03-10T05:10:15.179993Z","shell.execute_reply":"2025-03-10T05:10:15.185473Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def distillation_loss(student_logits, teacher_logits, labels, alpha=0.5, temperature=3.0):\n    \"\"\"\n    Compute the knowledge distillation loss.\n    - student_logits: Predictions from student\n    - teacher_logits: Predictions from teacher\n    - labels: True labels\n    - alpha: Balance between soft & hard labels\n    - temperature: Softens teacher logits\n    \"\"\"\n    soft_targets = nn.functional.softmax(teacher_logits / temperature, dim=1)\n    soft_loss = nn.functional.kl_div(\n        nn.functional.log_softmax(student_logits / temperature, dim=1), \n        soft_targets, reduction=\"batchmean\"\n    ) * (temperature ** 2)\n\n    hard_loss = nn.functional.cross_entropy(student_logits, labels)\n    \n    return alpha * soft_loss + (1 - alpha) * hard_loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T05:17:37.607495Z","iopub.execute_input":"2025-03-10T05:17:37.607805Z","iopub.status.idle":"2025-03-10T05:17:37.612545Z","shell.execute_reply.started":"2025-03-10T05:17:37.607783Z","shell.execute_reply":"2025-03-10T05:17:37.611759Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def train_student(teacher_model, student_model, train_loader, epochs=5):\n    criterion = distillation_loss\n    optimizer = optim.Adam(student_model.parameters(), lr=0.001)\n\n    teacher_model.eval()  # Teacher model is fixed\n    student_model.train()\n\n    for epoch in range(epochs):\n        total_loss = 0\n        for images, labels in train_loader:\n            with torch.no_grad():\n                teacher_outputs = teacher_model(images)  # Get teacher predictions\n            \n            student_outputs = student_model(images)\n            loss = criterion(student_outputs, teacher_outputs, labels)\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n        \n        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}\")\n\ntrain_student(teacher_model, student_model, train_loader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T05:18:49.669945Z","iopub.execute_input":"2025-03-10T05:18:49.670295Z","iopub.status.idle":"2025-03-10T05:19:57.571846Z","shell.execute_reply.started":"2025-03-10T05:18:49.670269Z","shell.execute_reply":"2025-03-10T05:19:57.570934Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 1.3424395718999\nEpoch 2, Loss: 0.49816228388977457\nEpoch 3, Loss: 0.28418930032169387\nEpoch 4, Loss: 0.20802542790452808\nEpoch 5, Loss: 0.17380074074845325\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"def evaluate(model, test_loader):\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for images, labels in test_loader:\n            outputs = model(images)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    \n    return correct / total\n\nteacher_acc = evaluate(teacher_model, test_loader)\nstudent_acc = evaluate(student_model, test_loader)\n\nprint(f\"Teacher Model Accuracy: {teacher_acc * 100:.2f}%\")\nprint(f\"Student Model Accuracy: {student_acc * 100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T05:20:54.647508Z","iopub.execute_input":"2025-03-10T05:20:54.647831Z","iopub.status.idle":"2025-03-10T05:20:58.403690Z","shell.execute_reply.started":"2025-03-10T05:20:54.647804Z","shell.execute_reply":"2025-03-10T05:20:58.402889Z"}},"outputs":[{"name":"stdout","text":"Teacher Model Accuracy: 96.59%\nStudent Model Accuracy: 96.42%\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}